---
title: Excercises in Barlow, Chapter 2
author: Ed Cashin
output: pdf_document
date: "`r Sys.Date()`"
fig_caption: true
---

This document is rendered in R using the *Rmarkdown* package.

```{r example_render, eval=F}
> rmarkdown::render('two.Rmd')
```

## Exercise 2.1

First, type in the data as $x$.

```{r first_vector}
x <- c(19, 18.7, 19.3, 19.2, 18.9, 19, 20.2, 19.9, 18.6, 19.4, 19.3, 18.8, 19.3, 19.2, 18.7, 18.5, 18.6, 19.7, 19.9, 20, 19.5, 19.4, 19.6, 20, 18.9)
```

Calculate the mean manually (somewhat manually), and verify it using
the base R function.

```{r}
sum(x)
length(x)
sum(x) / length(x)
mean(x)
```

Now calculate the standard deviation and verify with R base *sd*,
which uses Barlow's equation 2.11, with $N-1$ on the denominator.

```{r}
(x - mean(x))^2
sum((x - mean(x))^2)
sqrt(sum((x - mean(x))^2)/(length(x)-1))
sd(x)
```

## Exercise 2.2

Now add in the instructor's age of 37 to see how that affects the mean
and standard deviation.

```{r}
mean(c(x, 37))
sd(c(x, 37))
```

## Exercise 2.3

Below I create a function for the skew.

```{r}
skew <- function(x) { sum((x - mean(x))^3)/(length(x)*sd(x)^3) }
skew(x)
skew(c(x, 37))
```

For verifying that, I searched for calculation of skew in R and found
a blog about using the "moments" package.

http://www.r-bloggers.com/measures-of-skewness-and-kurtosis/

```{r}
library(moments)
skewness(x)
skew(x)
```

They're a bit different.  Subtracting one in the denominator isn't
quite enough to make them the same.

```{r}
skew2 <- function(x) { sum((x - mean(x))^3)/((length(x)-1)*sd(x)^3) }
skew2(x)
```

Looking at the code (shown below), I see that the *moments::skewness*
doesn't subtract one from the denominator in the standard deviation
calculation.

```{r}
skewness
```

## Exercise 2.4

The following contents are in file `grades.dat`.

```
classical quantum
22	  63
48	  39
76	  61
10	  30
22	  51
4	  44
68	  74
44	  78
10	  55
76	  58
14	  41
56	  69
```

The grades are loaded.

```{r}
df <- data.frame(read.table("grades.dat", header=T))
```

The *dplyr* library can do aggregations, and I want to get the hang of
it.  I also want to make that data "tidy", so I install *tidyr* and
use it.

```{r results="hide"}
library(dplyr)
library(tidyr)
```

I want the "classical" or "quantum" course specification to be a
categorical column in the data, and *gather* from *tidyr* does that.

```{r}
df %>% gather(course, grade, classical:quantum)
```

Then I can group by course and do summary statistics using
*dplyr::summarize*.

```{r}
df %>% gather(course, grade, classical:quantum) %>%
  group_by(course) %>% summarize(mean=mean(grade))
```

But I cannot think of a fancy way to do the covariance.  Base R's
*cov* function works on matrices and vectors.

```{r}
x <- df$classical
y <- df$quantum
cov(x, y)
```

By hand, Barlow's equation 2.19c is implemented below.

```{r}
mean(x*y) - (mean(x)*mean(y))
```

It's different from the results of `cov`, probably because of the
`method` parameter used by default.

```{r}
cov(x, y, method="kendall")
cov(x, y, method="pearson")
cov(x, y, method="spearman")
```

I guess not.

Here's the correlation by Barlow's 2.20b, checked with the base R
correlation function.

```{r}
(mean(x*y) - (mean(x)*mean(y))) / (sd(x)*sd(y))
cor(x, y)
```

## Exercise 2.6

I tried an old-fashioned stem and leaf plot to get a quick feel for
the way the histogram will look.

```{r}
(eighty <- read.table("80.dat"))
(eighty <- c(t(eighty)))
stem(eighty)
```

Use histogram in ggplot now.

```{r}
library(ggplot2)
qplot(x, data=data.frame(x=eighty), binwidth=10)
```

## Exercise 2.7

```{r}
mean(eighty)
median(eighty)
```

Using a for loop can find the most popular value.  First, create a
vector of the distinct values and a corresponding vector to store the
counts for each distinct value.

```{r}
(vals <- sort(unique(eighty)))
counts <- rep(0, length(vals))
```

Then loop and count, using the boolean indexing of R.  The mode is
seventy-nine.

```{r}
for (i in eighty) { counts[vals == i] = counts[vals == i] + 1 }
counts
max(counts)
vals[counts == max(counts)]
```

## Exercise 2.8

Standard deviation by hand and with base R function follows.

```{r}
sqrt(sum((eighty - mean(eighty))^2) / length(eighty))
sd(eighty)
```

For the FWHM, the maximum bin in a histogram is the largest count.
The base R *hist* function returns an object with the counts.

```{r}
hist(eighty)$counts
```

There are no bins with a height that is exactly half that of the
maximum bin, namely nine.  From the plot, though, it looks like the
histogram is spanning an interval of **forty** horizontal units when
measured nine vertical units above the base.

Equation 2.12, for times when a gaussian distribution is an
appropriate assumption, would have estimated a FWHM as shown below.

```{r}
2.35 * sd(eighty)
```

So yes, the estimate conforms to the hand-measured FWHM.
